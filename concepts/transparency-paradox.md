---
status: solid
area: [risk]
sources:
  - "Reimann & Schilke (2025)"
  - "Cheong et al. (2025)"
reviewed_by:
reviewed_date:
---

# Transparency Paradox

## What It Is

The disconnect between stated preferences for AI disclosure and actual behavior: people overwhelmingly say they want AI transparency (93.8%), yet consistently punish those who disclose (16-20% trust reduction).

## Why It Matters

Creates rational incentives to hide AI use. When honesty is systematically penalized, ethical norms around transparency become unsustainable. This shapes how professionals navigate AI disclosure in their work.

## Key Insight

The paradox operates at multiple levels:
- **Psychological**: We want to know, but the knowledge triggers negative judgments
- **Social**: Disclosure signals "my skills aren't enough" even when AI use is appropriate
- **Economic**: Getting caught is worse than disclosing, creating incentive to hide

## Evidence

Across 13 experiments with 5,000+ participants:
- Student trust in professors dropped 16% when AI grading was disclosed
- Investor confidence in firms fell 18% following AI advertising disclosures
- Client trust in designers declined 20% after AI disclosure
- Effect persisted regardless of respondent's own AI familiarity

## Vicious Cycle

```
Stigma exists
    ↓
People hide AI use (rational response)
    ↓
Public lacks examples of responsible AI use
    ↓
Stigma reinforced
    ↓
Cycle continues
```

## Related

- [[disclosure-penalty]] - the measured trust reduction
- [[fluency-bias]] - why we judge AI-assisted work differently
- [[authenticity]] - what disclosure threatens
