---
status: emerging
area: [preservation]
sources:
  - "Jarmołkowicz (2025)"
reviewed_by:
reviewed_date:
---

# Permission Structures

## What It Is

Explicit criteria for when heavy AI use is not just acceptable but appropriate—transforming the framework from constant vigilance into practical navigation.

## Why It Matters

The framework can feel like a source of guilt rather than guidance. Practitioners need to know: when IS heavy AI use appropriate without erosion risk? Permission structures provide triage, not just principles.

## Key Insight

**From practitioner feedback:**
> "Helping me triage would be more useful than principles to feel guilty about."

**Potential permission categories:**

| Category | Permission Condition | Rationale |
|----------|---------------------|-----------|
| **Routine execution** | Low-stakes, standardized tasks | No capacity at risk; efficiency is pure gain |
| **Time-critical delivery** | Emergency deadlines with restoration plan | Harm reduction better than burnout |
| **Outside core expertise** | Tasks peripheral to professional identity | Not where your value lives; preserve energy for what matters |
| **Scaffolded learning** | AI as tutor with active engagement | Tool-as-scaffold mode—building, not substituting |

**Questions still being explored:**
- How do you prevent "permission" from becoming rationalization for erosion?
- What distinguishes legitimate permission from self-deception?
- How does this interact with Green/Yellow/Red monitoring?

**Status:** Emerged from practitioner feedback. Fills real gap in framework usability. Needs development to prevent misuse.

## Related

- [[calibration]] - permission structures complement calibration
- [[green-yellow-red-monitoring]] - permissions interact with current state
- [[emergency-protocol]] - one form of permission structure
- [[strategic-alternation]] - permissions define when alternation isn't needed
