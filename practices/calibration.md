---
status: emerging
area: [preservation]
sources:
  - "Jarmołkowicz (2025)"
  - "Tsim & Gutoreva (2025)"
  - "Tankelevitch et al. (2024)"
reviewed_by:
reviewed_date:
---

# Calibration Practice

## What To Do

Before each task, consciously choose your AI engagement mode based on context.

## How To Do It

**Simple calibration (one question):**

"How much of ME does this task need?"

| Answer | Mode | What You Do |
|--------|------|-------------|
| My thinking | Think-First | Substantial solo work first. AI extends YOUR work. |
| My review | Partner | You seed direction, AI expands, you evaluate critically. |
| Just completion | Evaluate | AI generates, you verify. Only when you can judge quality. |

**Deep calibration (three questions):**

1. **What's my expertise level?**
   - Novice → Think-First 70%+
   - Intermediate → Balance Think-First and Partner
   - Expert → Can use Evaluate strategically

2. **What does this task require?**
   - High (novel, strategic, high stakes) → Think-First
   - Moderate (established patterns) → Partner/Evaluate
   - Low (routine) → Evaluate

3. **What's my current state?**
   - Check Green/Yellow/Red signals
   - Heavy recent AI use → more solo work

**When uncertain:** Default to Think-First.

**SCAN-based calibration (zone check):**

Before each task, locate yourself in SCAN zones:

| Question | Your Answer | Zone | Implication |
|----------|-------------|------|-------------|
| "Could I do this task myself?" | Yes, fully | **Complement** | AI assists, you lead |
| | Partly | **Aid** | AI scaffolds, you engage actively |
| | No | **Substitute** | ⚠️ Learning opportunity or delegation? |
| "Does this require human judgment?" | Yes | **Non-negotiable** | Minimize AI, you decide |

**Migration tracking:** Over time, the same task type should move S→A→C. If tasks are moving C→A, increase solo practice.

## Why It Works

Creates conscious choice point against default ease-seeking. Builds [[metacognition]] over time.

CHI 2024 research frames calibration as "metacognitive skill"—the psychological ability to monitor (assess quality of thinking) and control (adjust strategy based on assessment). This positions calibration as trainable through deliberate practice with feedback.

**Note on broken feedback loops:** Research shows feedback loops are broken with AI—users accept outputs, rarely discover if correct, so no calibration happens naturally. Unlike traditional skill development where errors provide correction signals, AI users often never learn when they've accepted flawed output. This means metacognitive accuracy doesn't naturally improve with AI experience—deliberate calibration practice is required.

## Related

- [[metacognition]] - calibration requires self-awareness
- [[strategic-alternation]] - calibration enables mode selection
- [[think-first]] - primary mode
- [[scan]] - zone-based calibration framework
- [[zone-of-proximal-development]] - theoretical basis
- [[upskilling-deskilling-paradox]] - what calibration prevents
- [[metacognitive-demand]] - theoretical framework
- [[jagged-frontier]] - what you're calibrating for
